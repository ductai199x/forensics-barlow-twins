{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 01:11:26.225285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:26.250301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:26.250414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, os.getcwd())\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(physical_devices)\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader, IterableDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(path, prefix=\"\", suffix=\"\", contains=\"\"):\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"{path} is not a valid directory.\")\n",
    "    files = []\n",
    "    for pre, dirs, basenames in os.walk(path):\n",
    "        for name in basenames:\n",
    "            if name.startswith(prefix) and name.endswith(suffix) and contains in name:\n",
    "                files.append(os.path.join(pre, name))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_STAT_DIR = \"/home/tai/1-workdir/1-deepfake-transformer/src/dataset_stuff/image_generator/db_stats\"\n",
    "PATCH_SIZE = 128\n",
    "ROOT_DB_DIR = f\"/media/nas2/misl_image_db_70_class\"\n",
    "TRAIN_DS_PATH = f\"{ROOT_DB_DIR}/train/{PATCH_SIZE}\"\n",
    "VAL_DS_PATH = f\"{ROOT_DB_DIR}/val/{PATCH_SIZE}\"\n",
    "\n",
    "NUM_CLASSES = 70\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_recs = get_all_files(TRAIN_DS_PATH, suffix=\".tfrecord\")\n",
    "val_recs = get_all_files(VAL_DS_PATH, suffix=\".tfrecord\")\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "image_feature_description = {\n",
    "    \"raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 01:11:27.507739: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 01:11:27.509371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:27.509612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:27.509793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:28.022499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:28.022625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:28.022723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 01:11:28.022815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def _parse_image_function(example_proto):\n",
    "    parsed_feature = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.io.parse_tensor(parsed_feature[\"raw\"], tf.float32)\n",
    "    image = tf.reshape(image, [PATCH_SIZE, PATCH_SIZE, 3])\n",
    "    label = tf.cast(parsed_feature[\"label\"], tf.int64)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "raw_train_set = tf.data.Dataset.from_tensor_slices(train_recs).interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x).map(_parse_image_function, num_parallel_calls=AUTOTUNE),\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "    cycle_length=NUM_CLASSES,\n",
    "    block_length=2,\n",
    ")\n",
    "raw_val_set = tf.data.TFRecordDataset(val_recs).map(_parse_image_function)\n",
    "\n",
    "\n",
    "train_tfds = raw_train_set.shuffle(buffer_size=BATCH_SIZE * 2).batch(batch_size=BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "val_tfds = raw_val_set.batch(batch_size=BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "    def process_data(self, generator):\n",
    "        for image, label in generator:\n",
    "            image = torch.from_numpy(image.numpy()).permute(0, 3, 1, 2)  # BHWC->BCHW\n",
    "            label = torch.from_numpy(label.numpy()).long()\n",
    "            yield image, label\n",
    "\n",
    "    def get_stream(self, generator):\n",
    "        return self.process_data(generator)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_itds = MyIterableDataset(train_tfds)\n",
    "val_itds = MyIterableDataset(val_tfds)\n",
    "train_dl = DataLoader(train_itds, batch_size=None, num_workers=0)\n",
    "val_dl = DataLoader(val_itds, batch_size=None, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mislnet import MISLnetPLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"input_size\": (128, 128),\n",
    "    \"output_dim\": 1024,\n",
    "    \"num_classes\": 70,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.95,\n",
    "    \"decay_rate\": 0.75,\n",
    "    \"decay_step\": 4,\n",
    "}\n",
    "\n",
    "model = MISLnetPLWrapper(config)\n",
    "model_name = \"mislnet-128-1024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:45: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=100)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:187: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "prev_ckpt = None\n",
    "prev_ckpt = \"/home/tai/1-workdir/5-forensics-barlow-twins/src/lightning_logs/version_1/checkpoints/mislnet-128-1024=0-epoch=184-val_loss=0.9704.ckpt\"\n",
    "\n",
    "resume = True\n",
    "if prev_ckpt:\n",
    "    model = model.load_from_checkpoint(prev_ckpt, args=config)\n",
    "\n",
    "version = 1\n",
    "monitor_metric = \"val_loss\"\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), version=version, name=\"src/lightning_logs\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "model_ckpt = ModelCheckpoint(\n",
    "    dirpath=f\"src/lightning_logs/version_{version}/checkpoints\",\n",
    "    monitor=monitor_metric,\n",
    "    filename=f\"{{{model_name}}}-{{epoch:02d}}-{{{monitor_metric}:.4f}}\",\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=200,\n",
    "    resume_from_checkpoint=prev_ckpt if resume else None,\n",
    "    progress_bar_refresh_rate=100,\n",
    "    weights_summary=\"full\",\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor, model_ckpt],\n",
    "    fast_dev_run=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1905: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v1.7. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Restoring states from the checkpoint path at /home/tai/1-workdir/5-forensics-barlow-twins/src/lightning_logs/version_1/checkpoints/mislnet-128-1024=0-epoch=184-val_loss=0.9704.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:247: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint file at /home/tai/1-workdir/5-forensics-barlow-twins/src/lightning_logs/version_1/checkpoints/mislnet-128-1024=0-epoch=184-val_loss=0.9704.ckpt\n",
      "\n",
      "   | Name           | Type        | Params | In sizes         | Out sizes       \n",
      "--------------------------------------------------------------------------------------\n",
      "0  | model          | MISLnet     | 3.5 M  | [1, 3, 128, 128] | [1, 70]         \n",
      "1  | model.model    | Sequential  | 3.5 M  | [1, 3, 128, 128] | [1, 70]         \n",
      "2  | model.model.0  | Conv2d      | 228    | [1, 3, 128, 128] | [1, 3, 124, 124]\n",
      "3  | model.model.1  | Conv2d      | 14.2 K | [1, 3, 124, 124] | [1, 96, 64, 64] \n",
      "4  | model.model.2  | BatchNorm2d | 192    | [1, 96, 64, 64]  | [1, 96, 64, 64] \n",
      "5  | model.model.3  | Tanh        | 0      | [1, 96, 64, 64]  | [1, 96, 64, 64] \n",
      "6  | model.model.4  | MaxPool2d   | 0      | [1, 96, 64, 64]  | [1, 96, 32, 32] \n",
      "7  | model.model.5  | Conv2d      | 153 K  | [1, 96, 32, 32]  | [1, 64, 32, 32] \n",
      "8  | model.model.6  | BatchNorm2d | 128    | [1, 64, 32, 32]  | [1, 64, 32, 32] \n",
      "9  | model.model.7  | Tanh        | 0      | [1, 64, 32, 32]  | [1, 64, 32, 32] \n",
      "10 | model.model.8  | MaxPool2d   | 0      | [1, 64, 32, 32]  | [1, 64, 16, 16] \n",
      "11 | model.model.9  | Conv2d      | 102 K  | [1, 64, 16, 16]  | [1, 64, 16, 16] \n",
      "12 | model.model.10 | BatchNorm2d | 128    | [1, 64, 16, 16]  | [1, 64, 16, 16] \n",
      "13 | model.model.11 | Tanh        | 0      | [1, 64, 16, 16]  | [1, 64, 16, 16] \n",
      "14 | model.model.12 | MaxPool2d   | 0      | [1, 64, 16, 16]  | [1, 64, 7, 7]   \n",
      "15 | model.model.13 | Conv2d      | 8.3 K  | [1, 64, 7, 7]    | [1, 128, 7, 7]  \n",
      "16 | model.model.14 | BatchNorm2d | 256    | [1, 128, 7, 7]   | [1, 128, 7, 7]  \n",
      "17 | model.model.15 | Tanh        | 0      | [1, 128, 7, 7]   | [1, 128, 7, 7]  \n",
      "18 | model.model.16 | AvgPool2d   | 0      | [1, 128, 7, 7]   | [1, 128, 4, 4]  \n",
      "19 | model.model.17 | Flatten     | 0      | [1, 128, 4, 4]   | [1, 2048]       \n",
      "20 | model.model.18 | Linear      | 2.1 M  | [1, 2048]        | [1, 1024]       \n",
      "21 | model.model.19 | Tanh        | 0      | [1, 1024]        | [1, 1024]       \n",
      "22 | model.model.20 | Linear      | 1.0 M  | [1, 1024]        | [1, 1024]       \n",
      "23 | model.model.21 | Tanh        | 0      | [1, 1024]        | [1, 1024]       \n",
      "24 | model.model.22 | Linear      | 71.8 K | [1, 1024]        | [1, 70]         \n",
      "25 | train_acc      | Accuracy    | 0      | ?                | ?               \n",
      "26 | val_acc        | Accuracy    | 0      | ?                | ?               \n",
      "--------------------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "13.996    Total estimated model params size (MB)\n",
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/tai/1-workdir/5-forensics-barlow-twins/src/lightning_logs/version_1/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9afc1e50b444adb8759a11e1d0e945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/tai/1-workdir/pyt_tf2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af48cd4c00234c9a8cc4d22f2817ec6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e69cebe8e6440868c5a52279bf5af2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185, global step 5813987: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e9790e0a447d1b73b041455f1bb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186, global step 5845245: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ac02b244244e509151c4774f3c113b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187, global step 5876503: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49749f9518524a5fbe6c3fd42ae822a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188, global step 5907761: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec504d1b94744c99a08ac68aab7981b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189, global step 5939019: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37747fda18504cc7b5a48448b6dc20cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190, global step 5970277: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388867214b3e4c35b679f825a0b64b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191, global step 6001535: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12ac5adbe8c4b15bf408b2c77940a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192, global step 6032793: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f463b1782f5d4ca78e51bac07fe5af99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193, global step 6064051: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b139d6131b63449a86a1c4211b5bbef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194, global step 6095309: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc17cd91ba648a48e5a61a535ac0c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195, global step 6126567: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e965b1249548febf1bf1aea3f8bf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196, global step 6157825: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314024ec87a34971bf48ce08057574da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197, global step 6189083: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a6e9a94b849989203cdd4f5087d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198, global step 6220341: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21233f15aff04a0da0c82c994cec0c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199, global step 6251599: val_loss was not in top 1\n",
      "Saving latest checkpoint...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dl, val_dl)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0421650a8a5845f713c56c3ba4f436fc22593ad99b656f8436ea71c2ab26d6c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('pyt_tf2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
